{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1BVxtJSDZ4H3HRBio1exd"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HAKjLwJ4n3DH"
      },
      "outputs": [],
      "source": [
        "''' In this project, we are creating a web scraping tool to collect information about Data Science job postings on Linkedin.\n",
        "    We aim to extract information about company names, job titles, location, seniority levels. and skills required.\n",
        "    We will be collecting this information from 1000 jobs.\n",
        "'''\n",
        "\n",
        "# We first import the required libraries.\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import time\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we create the main function which extracts the necessary information about each job posting\n",
        "\n",
        "def webscrapping_one_page(url, headers):\n",
        "  try:\n",
        "    html_text = requests.get(url, headers=headers).text\n",
        "    soup = BeautifulSoup(html_text, 'lxml')\n",
        "    jobs = soup.find_all(\"li\")\n",
        "\n",
        "    links = []\n",
        "    for job in jobs:\n",
        "      link = job.find(\"a\", {\"class\":\"base-card__full-link absolute top-0 right-0 bottom-0 left-0 p-0 z-[2]\", \"href\":True})[\"href\"]\n",
        "      links.append(link)\n",
        "    \n",
        "    for link in links:\n",
        "      time.sleep(1)\n",
        "      headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15\"}\n",
        "      html_text = requests.get(link, headers=headers).text\n",
        "      soup = BeautifulSoup(html_text, 'lxml')\n",
        "      company_name = soup.find(\"a\", class_=\"topcard__org-name-link topcard__flavor--black-link\").text.strip()\n",
        "      job_title = soup.find(\"h1\", class_=\"top-card-layout__title font-sans text-lg papabear:text-xl font-bold leading-open text-color-text mb-0 topcard__title\").text\n",
        "      location = soup.find(\"span\", class_=\"topcard__flavor topcard__flavor--bullet\").text.strip()\n",
        "      seniority_level = soup.find(\"span\", class_=\"description__job-criteria-text description__job-criteria-text--criteria\").text.strip()\n",
        "      description = soup.find(\"div\", class_=\"show-more-less-html__markup\")\n",
        "      description = str(description).casefold()\n",
        "      if \"python\".casefold() in description:\n",
        "        python = True\n",
        "      else:\n",
        "        python = False\n",
        "      if \"sql\".casefold() in description:\n",
        "        sql = True\n",
        "      else:\n",
        "        sql = False\n",
        "      if \" R \".casefold() in description:\n",
        "        R = True\n",
        "      else:\n",
        "        R = False\n",
        "      if \"excel\".casefold() in description:\n",
        "        excel = True\n",
        "      else:\n",
        "        excel = False\n",
        "      if \"tableau\".casefold() in description:\n",
        "        tableau = True\n",
        "      else:\n",
        "        tableau = False\n",
        "      if \"power bi\".casefold() in description:\n",
        "        power_bi = True\n",
        "      else:\n",
        "        power_bi = False\n",
        "      if \"machine learning\".casefold() in description:\n",
        "        ml = True\n",
        "      else:\n",
        "        ml = False\n",
        "      data = [company_name,job_title,location,seniority_level,python,sql,R,excel,tableau,power_bi,ml]\n",
        "      print(data)\n",
        "      with open('linkedinwebscraper.csv', 'a+', newline='', encoding='UTF8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(data)  \n",
        "  except:\n",
        "    print('error')"
      ],
      "metadata": {
        "id": "YgcUBShOzxZC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table = ['Company','Job_Title', 'Location', 'Seniority_level', 'Python',\n",
        "         'SQL', 'R', 'Excel', 'Tableau', 'Power_BI', 'Machine_Learning']\n",
        "with open('linkedinwebscraper.csv', 'w', newline='', encoding='UTF8') as f:\n",
        "  writer = csv.writer(f)\n",
        "  writer.writerow(table)  \n",
        "\n",
        "url = 'https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=Data%20Scientist&location=canada&geoId=&trk=public_jobs_jobs-search-bar_search-submit&position=1&pageNum=0&start='\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15\"}\n",
        "\n",
        "# Now since there are 25 jobs per page, in order for us to extract the desired amount of jobs (in this case 1000),\n",
        "# we need to loop through 1000/25 = 40 pages.\n",
        "\n",
        "for page in range(40):\n",
        "\n",
        "  webscrapping_one_page(url+str(page*25), headers)"
      ],
      "metadata": {
        "id": "7mSFZhs71-WU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gH5mxRWm9BhI"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}